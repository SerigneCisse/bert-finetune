{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU selection\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "from tensorflow.compat.v1.keras import layers\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import bert.model\n",
    "import bert.utils\n",
    "import bert.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTLARGE     = False\n",
    "USE_AMP       = True\n",
    "USE_XLA       = True\n",
    "MAX_SEQ_LEN   = 128\n",
    "LEARNING_RATE = 1e-5\n",
    "TUNE_LAYERS   = -1\n",
    "DROPOUT_RATE  = 0.9\n",
    "BATCH_SIZE    = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PORTION = 0.001\n",
    "\n",
    "if BERTLARGE:\n",
    "    BERT_PATH = \"https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1\"\n",
    "    H_SIZE = 1024\n",
    "else:\n",
    "    BERT_PATH = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "    H_SIZE = 768\n",
    "\n",
    "sess = tf.Session()\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = bert.model.create_tokenizer_from_hub_module(BERT_PATH, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training set from: /home/jovyan/.keras/datasets/ag_news\n",
      "Examples: 120000 Classes: 4\n",
      "Number of training examples: 120\n",
      "Loaded test set from: /home/jovyan/.keras/datasets/ag_news\n",
      "Examples: 7600 Classes: 4\n"
     ]
    }
   ],
   "source": [
    "train_text, train_label, num_classes = bert.utils.load_ag_news_dataset(max_seq_len=MAX_SEQ_LEN,\n",
    "                                                                       test=False)\n",
    "\n",
    "if DATASET_PORTION < 1:\n",
    "    num_examples = int(len(train_label) * DATASET_PORTION)\n",
    "    _, train_text, _, train_label= train_test_split(train_text, train_label, test_size=DATASET_PORTION, stratify=train_label)\n",
    "else:\n",
    "    num_examples = len(train_label)\n",
    "\n",
    "train_label = np.asarray(train_label)\n",
    "feat = bert.model.convert_text_to_features(tokenizer, train_text, train_label, max_seq_length=MAX_SEQ_LEN, verbose=False)\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = feat\n",
    "\n",
    "print(\"Number of training examples:\", len(train_labels))\n",
    "\n",
    "examples, labels, num_classes = bert.utils.load_ag_news_dataset(max_seq_len=MAX_SEQ_LEN,\n",
    "                                                                test=True)\n",
    "labels = np.asarray(labels)\n",
    "feat = bert.model.convert_text_to_features(tokenizer, examples, labels, max_seq_length=MAX_SEQ_LEN, verbose=False)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels) = feat\n",
    "\n",
    "test_input_ids, test_input_masks, test_segment_ids, test_labels = shuffle(test_input_ids,\n",
    "                                                                          test_input_masks,\n",
    "                                                                          test_segment_ids,\n",
    "                                                                          test_labels)\n",
    "\n",
    "test_set = ([test_input_ids, test_input_masks, test_segment_ids], test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0911 03:20:10.069056 139890185373504 deprecation_wrapper.py:119] From /home/jovyan/bert-finetune/bert/model.py:121: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n",
      "W0911 03:20:19.553777 139890185373504 nn_ops.py:4248] Large dropout rate: 0.9 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0911 03:20:19.571656 139890185373504 deprecation.py:506] From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4)            3076        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,107,966\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 622,650\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    tf.keras.backend.clear_session()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    if USE_XLA:\n",
    "        opt_level = tf.OptimizerOptions.ON_1\n",
    "        tf.enable_resource_variables()\n",
    "    else:\n",
    "        opt_level = tf.OptimizerOptions.OFF\n",
    "    config.graph_options.optimizer_options.global_jit_level = opt_level\n",
    "    config.graph_options.rewrite_options.auto_mixed_precision = USE_AMP\n",
    "    sess = tf.Session(config=config)\n",
    "    tf.keras.backend.set_session(sess)\n",
    "    \n",
    "    if USE_AMP:\n",
    "        tf.keras.mixed_precision.experimental.set_policy('infer_float32_vars')\n",
    "\n",
    "    in_id = layers.Input(shape=(MAX_SEQ_LEN,), name=\"input_ids\")\n",
    "    in_mask = layers.Input(shape=(MAX_SEQ_LEN,), name=\"input_masks\")\n",
    "    in_segment = layers.Input(shape=(MAX_SEQ_LEN,), name=\"segment_ids\")\n",
    "    in_bert = [in_id, in_mask, in_segment]\n",
    "    l_bert = bert.model.BERT(fine_tune_layers=TUNE_LAYERS,\n",
    "                             bert_path=BERT_PATH,\n",
    "                             return_sequence=False,\n",
    "                             output_size=H_SIZE,\n",
    "                             debug=False)(in_bert)\n",
    "    l_drop_2 = layers.Dropout(rate=DROPOUT_RATE)(l_bert)\n",
    "    out_pred = layers.Dense(num_classes, activation=\"softmax\")(l_drop_2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=in_bert, outputs=out_pred)\n",
    "\n",
    "    opt = bert.optimizer.RAdam(lr=LEARNING_RATE)\n",
    "\n",
    "    if USE_AMP:\n",
    "        opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, \"dynamic\")\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    warmup_steps = 26000\n",
    "    warmup_epochs = warmup_steps//num_examples\n",
    "    if epoch < warmup_epochs:\n",
    "        return LEARNING_RATE*(epoch/warmup_epochs)\n",
    "    else:\n",
    "        return LEARNING_RATE\n",
    "    \n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks_list = [lr_schedule, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 7600 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0911 03:20:19.947500 139890185373504 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "120/120 - 65s - loss: 3.0841 - acc: 0.2833 - val_loss: 2.1188 - val_acc: 0.2489\n",
      "Epoch 2/1000\n",
      "120/120 - 39s - loss: 3.7698 - acc: 0.2500 - val_loss: 2.1187 - val_acc: 0.2489\n",
      "Epoch 3/1000\n",
      "120/120 - 5s - loss: 3.1664 - acc: 0.2917 - val_loss: 2.1186 - val_acc: 0.2489\n",
      "Epoch 4/1000\n",
      "120/120 - 5s - loss: 3.3819 - acc: 0.2583 - val_loss: 2.1183 - val_acc: 0.2489\n",
      "Epoch 5/1000\n",
      "120/120 - 5s - loss: 3.2093 - acc: 0.2167 - val_loss: 2.1178 - val_acc: 0.2489\n",
      "Epoch 6/1000\n",
      "120/120 - 5s - loss: 3.3698 - acc: 0.2583 - val_loss: 2.1170 - val_acc: 0.2489\n",
      "Epoch 7/1000\n",
      "120/120 - 5s - loss: 2.6571 - acc: 0.2583 - val_loss: 2.1159 - val_acc: 0.2489\n",
      "Epoch 8/1000\n",
      "120/120 - 5s - loss: 3.4120 - acc: 0.2500 - val_loss: 2.1146 - val_acc: 0.2489\n",
      "Epoch 9/1000\n",
      "120/120 - 5s - loss: 3.3430 - acc: 0.2667 - val_loss: 2.1129 - val_acc: 0.2488\n",
      "Epoch 10/1000\n",
      "120/120 - 5s - loss: 3.5564 - acc: 0.2250 - val_loss: 2.1108 - val_acc: 0.2489\n",
      "Epoch 11/1000\n",
      "120/120 - 5s - loss: 3.2916 - acc: 0.2500 - val_loss: 2.1082 - val_acc: 0.2489\n",
      "Epoch 12/1000\n",
      "120/120 - 5s - loss: 3.1074 - acc: 0.3000 - val_loss: 2.1051 - val_acc: 0.2489\n",
      "Epoch 13/1000\n",
      "120/120 - 5s - loss: 3.5174 - acc: 0.2333 - val_loss: 2.1017 - val_acc: 0.2488\n",
      "Epoch 14/1000\n",
      "120/120 - 5s - loss: 3.2059 - acc: 0.2500 - val_loss: 2.0996 - val_acc: 0.2487\n",
      "Epoch 15/1000\n",
      "120/120 - 5s - loss: 3.3913 - acc: 0.2667 - val_loss: 2.0959 - val_acc: 0.2487\n",
      "Epoch 16/1000\n",
      "120/120 - 5s - loss: 3.2797 - acc: 0.2250 - val_loss: 2.0914 - val_acc: 0.2487\n",
      "Epoch 17/1000\n",
      "120/120 - 5s - loss: 2.9154 - acc: 0.3333 - val_loss: 2.0862 - val_acc: 0.2487\n",
      "Epoch 18/1000\n",
      "120/120 - 5s - loss: 3.2091 - acc: 0.2583 - val_loss: 2.0804 - val_acc: 0.2488\n",
      "Epoch 19/1000\n",
      "120/120 - 5s - loss: 3.4296 - acc: 0.2583 - val_loss: 2.0743 - val_acc: 0.2488\n",
      "Epoch 20/1000\n",
      "120/120 - 5s - loss: 3.3943 - acc: 0.2833 - val_loss: 2.0672 - val_acc: 0.2488\n",
      "Epoch 21/1000\n",
      "120/120 - 5s - loss: 2.6739 - acc: 0.3083 - val_loss: 2.0593 - val_acc: 0.2487\n",
      "Epoch 22/1000\n",
      "120/120 - 5s - loss: 3.1879 - acc: 0.2500 - val_loss: 2.0508 - val_acc: 0.2487\n",
      "Epoch 23/1000\n",
      "120/120 - 5s - loss: 2.9304 - acc: 0.2750 - val_loss: 2.0420 - val_acc: 0.2487\n",
      "Epoch 24/1000\n",
      "120/120 - 5s - loss: 3.2932 - acc: 0.2250 - val_loss: 2.0329 - val_acc: 0.2484\n",
      "Epoch 25/1000\n",
      "120/120 - 6s - loss: 3.2632 - acc: 0.2583 - val_loss: 2.0223 - val_acc: 0.2486\n",
      "Epoch 26/1000\n",
      "120/120 - 5s - loss: 3.4144 - acc: 0.2167 - val_loss: 2.0106 - val_acc: 0.2483\n",
      "Epoch 27/1000\n",
      "120/120 - 5s - loss: 2.5573 - acc: 0.3250 - val_loss: 1.9978 - val_acc: 0.2479\n",
      "Epoch 28/1000\n",
      "120/120 - 5s - loss: 3.1024 - acc: 0.2750 - val_loss: 1.9840 - val_acc: 0.2480\n",
      "Epoch 29/1000\n",
      "120/120 - 5s - loss: 3.2086 - acc: 0.2167 - val_loss: 1.9692 - val_acc: 0.2487\n",
      "Epoch 30/1000\n",
      "120/120 - 5s - loss: 3.1760 - acc: 0.2917 - val_loss: 1.9539 - val_acc: 0.2495\n",
      "Epoch 31/1000\n",
      "120/120 - 5s - loss: 3.1454 - acc: 0.2500 - val_loss: 1.9363 - val_acc: 0.2495\n",
      "Epoch 32/1000\n",
      "120/120 - 5s - loss: 2.9847 - acc: 0.3000 - val_loss: 1.9181 - val_acc: 0.2495\n",
      "Epoch 33/1000\n",
      "120/120 - 5s - loss: 3.1051 - acc: 0.2917 - val_loss: 1.9007 - val_acc: 0.2488\n",
      "Epoch 34/1000\n",
      "120/120 - 5s - loss: 2.9197 - acc: 0.2333 - val_loss: 1.8808 - val_acc: 0.2503\n",
      "Epoch 35/1000\n",
      "120/120 - 5s - loss: 2.8450 - acc: 0.2667 - val_loss: 1.8604 - val_acc: 0.2507\n",
      "Epoch 36/1000\n",
      "120/120 - 5s - loss: 3.1685 - acc: 0.2333 - val_loss: 1.8394 - val_acc: 0.2513\n",
      "Epoch 37/1000\n",
      "120/120 - 5s - loss: 2.7268 - acc: 0.3083 - val_loss: 1.8198 - val_acc: 0.2522\n",
      "Epoch 38/1000\n",
      "120/120 - 5s - loss: 2.4054 - acc: 0.3500 - val_loss: 1.8014 - val_acc: 0.2534\n",
      "Epoch 39/1000\n",
      "120/120 - 5s - loss: 2.8627 - acc: 0.2333 - val_loss: 1.7838 - val_acc: 0.2547\n",
      "Epoch 40/1000\n",
      "120/120 - 5s - loss: 2.5528 - acc: 0.3333 - val_loss: 1.7667 - val_acc: 0.2562\n",
      "Epoch 41/1000\n",
      "120/120 - 5s - loss: 2.8029 - acc: 0.2500 - val_loss: 1.7476 - val_acc: 0.2578\n",
      "Epoch 42/1000\n",
      "120/120 - 5s - loss: 2.8448 - acc: 0.2500 - val_loss: 1.7263 - val_acc: 0.2612\n",
      "Epoch 43/1000\n",
      "120/120 - 5s - loss: 2.4414 - acc: 0.3000 - val_loss: 1.7047 - val_acc: 0.2653\n",
      "Epoch 44/1000\n",
      "120/120 - 5s - loss: 2.6103 - acc: 0.2333 - val_loss: 1.6841 - val_acc: 0.2700\n",
      "Epoch 45/1000\n",
      "120/120 - 5s - loss: 2.4553 - acc: 0.3417 - val_loss: 1.6649 - val_acc: 0.2757\n",
      "Epoch 46/1000\n",
      "120/120 - 5s - loss: 2.4335 - acc: 0.3250 - val_loss: 1.6490 - val_acc: 0.2792\n",
      "Epoch 47/1000\n",
      "120/120 - 5s - loss: 2.4659 - acc: 0.3083 - val_loss: 1.6346 - val_acc: 0.2832\n",
      "Epoch 48/1000\n",
      "120/120 - 5s - loss: 2.3620 - acc: 0.3500 - val_loss: 1.6217 - val_acc: 0.2863\n",
      "Epoch 49/1000\n",
      "120/120 - 5s - loss: 2.4653 - acc: 0.3000 - val_loss: 1.6074 - val_acc: 0.2922\n",
      "Epoch 50/1000\n",
      "120/120 - 5s - loss: 2.6657 - acc: 0.2667 - val_loss: 1.5931 - val_acc: 0.2946\n",
      "Epoch 51/1000\n",
      "120/120 - 5s - loss: 2.4882 - acc: 0.3167 - val_loss: 1.5797 - val_acc: 0.2980\n",
      "Epoch 52/1000\n",
      "120/120 - 5s - loss: 2.4659 - acc: 0.3083 - val_loss: 1.5682 - val_acc: 0.2992\n",
      "Epoch 53/1000\n",
      "120/120 - 5s - loss: 2.3750 - acc: 0.3167 - val_loss: 1.5578 - val_acc: 0.3029\n",
      "Epoch 54/1000\n",
      "120/120 - 5s - loss: 2.6042 - acc: 0.2667 - val_loss: 1.5483 - val_acc: 0.3051\n",
      "Epoch 55/1000\n",
      "120/120 - 5s - loss: 1.8711 - acc: 0.3250 - val_loss: 1.5379 - val_acc: 0.3099\n",
      "Epoch 56/1000\n",
      "120/120 - 5s - loss: 2.6836 - acc: 0.2750 - val_loss: 1.5302 - val_acc: 0.3124\n",
      "Epoch 57/1000\n",
      "120/120 - 5s - loss: 2.4536 - acc: 0.2917 - val_loss: 1.5238 - val_acc: 0.3147\n",
      "Epoch 58/1000\n",
      "120/120 - 5s - loss: 2.6693 - acc: 0.2667 - val_loss: 1.5150 - val_acc: 0.3157\n",
      "Epoch 59/1000\n",
      "120/120 - 5s - loss: 2.3368 - acc: 0.2833 - val_loss: 1.5053 - val_acc: 0.3189\n",
      "Epoch 60/1000\n",
      "120/120 - 5s - loss: 2.3017 - acc: 0.3417 - val_loss: 1.4976 - val_acc: 0.3200\n",
      "Epoch 61/1000\n",
      "120/120 - 5s - loss: 2.5484 - acc: 0.3000 - val_loss: 1.4890 - val_acc: 0.3224\n",
      "Epoch 62/1000\n",
      "120/120 - 5s - loss: 2.4203 - acc: 0.3167 - val_loss: 1.4806 - val_acc: 0.3246\n",
      "Epoch 63/1000\n",
      "120/120 - 5s - loss: 2.2028 - acc: 0.3083 - val_loss: 1.4737 - val_acc: 0.3301\n",
      "Epoch 64/1000\n",
      "120/120 - 5s - loss: 2.3835 - acc: 0.2750 - val_loss: 1.4667 - val_acc: 0.3325\n",
      "Epoch 65/1000\n",
      "120/120 - 5s - loss: 2.1252 - acc: 0.3583 - val_loss: 1.4597 - val_acc: 0.3346\n",
      "Epoch 66/1000\n",
      "120/120 - 5s - loss: 2.2797 - acc: 0.3000 - val_loss: 1.4534 - val_acc: 0.3380\n",
      "Epoch 67/1000\n",
      "120/120 - 5s - loss: 2.5251 - acc: 0.3250 - val_loss: 1.4483 - val_acc: 0.3392\n",
      "Epoch 68/1000\n",
      "120/120 - 5s - loss: 2.4310 - acc: 0.2917 - val_loss: 1.4438 - val_acc: 0.3400\n",
      "Epoch 69/1000\n",
      "120/120 - 5s - loss: 2.1569 - acc: 0.3000 - val_loss: 1.4393 - val_acc: 0.3414\n",
      "Epoch 70/1000\n",
      "120/120 - 5s - loss: 2.2015 - acc: 0.3333 - val_loss: 1.4348 - val_acc: 0.3429\n",
      "Epoch 71/1000\n",
      "120/120 - 5s - loss: 2.2550 - acc: 0.3583 - val_loss: 1.4298 - val_acc: 0.3433\n",
      "Epoch 72/1000\n",
      "120/120 - 5s - loss: 2.1723 - acc: 0.3333 - val_loss: 1.4248 - val_acc: 0.3439\n",
      "Epoch 73/1000\n",
      "120/120 - 5s - loss: 2.3180 - acc: 0.2667 - val_loss: 1.4201 - val_acc: 0.3447\n",
      "Epoch 74/1000\n",
      "120/120 - 5s - loss: 2.3749 - acc: 0.3333 - val_loss: 1.4150 - val_acc: 0.3439\n",
      "Epoch 75/1000\n",
      "120/120 - 5s - loss: 2.0517 - acc: 0.4083 - val_loss: 1.4105 - val_acc: 0.3462\n",
      "Epoch 76/1000\n",
      "120/120 - 5s - loss: 2.0260 - acc: 0.3333 - val_loss: 1.4058 - val_acc: 0.3471\n",
      "Epoch 77/1000\n",
      "120/120 - 5s - loss: 1.9457 - acc: 0.3833 - val_loss: 1.4001 - val_acc: 0.3475\n",
      "Epoch 78/1000\n",
      "120/120 - 5s - loss: 2.0868 - acc: 0.3083 - val_loss: 1.3946 - val_acc: 0.3478\n",
      "Epoch 79/1000\n",
      "120/120 - 5s - loss: 2.3416 - acc: 0.3583 - val_loss: 1.3902 - val_acc: 0.3461\n",
      "Epoch 80/1000\n",
      "120/120 - 5s - loss: 2.0675 - acc: 0.2833 - val_loss: 1.3881 - val_acc: 0.3455\n",
      "Epoch 81/1000\n",
      "120/120 - 5s - loss: 1.9802 - acc: 0.4083 - val_loss: 1.3841 - val_acc: 0.3445\n",
      "Epoch 82/1000\n",
      "120/120 - 5s - loss: 2.1302 - acc: 0.3583 - val_loss: 1.3789 - val_acc: 0.3447\n",
      "Epoch 83/1000\n",
      "120/120 - 5s - loss: 1.9057 - acc: 0.3583 - val_loss: 1.3720 - val_acc: 0.3470\n",
      "Epoch 84/1000\n",
      "120/120 - 5s - loss: 1.8525 - acc: 0.3750 - val_loss: 1.3632 - val_acc: 0.3493\n",
      "Epoch 85/1000\n",
      "120/120 - 5s - loss: 1.9538 - acc: 0.3500 - val_loss: 1.3539 - val_acc: 0.3505\n",
      "Epoch 86/1000\n",
      "120/120 - 5s - loss: 2.0553 - acc: 0.4167 - val_loss: 1.3482 - val_acc: 0.3555\n",
      "Epoch 87/1000\n",
      "120/120 - 5s - loss: 1.9817 - acc: 0.3083 - val_loss: 1.3450 - val_acc: 0.3584\n",
      "Epoch 88/1000\n",
      "120/120 - 5s - loss: 1.8223 - acc: 0.4083 - val_loss: 1.3409 - val_acc: 0.3595\n",
      "Epoch 89/1000\n",
      "120/120 - 5s - loss: 1.8503 - acc: 0.3917 - val_loss: 1.3337 - val_acc: 0.3604\n",
      "Epoch 90/1000\n",
      "120/120 - 5s - loss: 1.5745 - acc: 0.4167 - val_loss: 1.3264 - val_acc: 0.3618\n",
      "Epoch 91/1000\n",
      "120/120 - 5s - loss: 1.9899 - acc: 0.3833 - val_loss: 1.3199 - val_acc: 0.3608\n",
      "Epoch 92/1000\n",
      "120/120 - 5s - loss: 1.9458 - acc: 0.3583 - val_loss: 1.3164 - val_acc: 0.3621\n",
      "Epoch 93/1000\n",
      "120/120 - 5s - loss: 1.7287 - acc: 0.4333 - val_loss: 1.3126 - val_acc: 0.3616\n",
      "Epoch 94/1000\n",
      "120/120 - 5s - loss: 1.6627 - acc: 0.3750 - val_loss: 1.3091 - val_acc: 0.3616\n",
      "Epoch 95/1000\n",
      "120/120 - 5s - loss: 1.9304 - acc: 0.3167 - val_loss: 1.3043 - val_acc: 0.3625\n",
      "Epoch 96/1000\n",
      "120/120 - 5s - loss: 1.9992 - acc: 0.3750 - val_loss: 1.3003 - val_acc: 0.3642\n",
      "Epoch 97/1000\n",
      "120/120 - 5s - loss: 1.8072 - acc: 0.3750 - val_loss: 1.2965 - val_acc: 0.3636\n",
      "Epoch 98/1000\n",
      "120/120 - 5s - loss: 1.9411 - acc: 0.3333 - val_loss: 1.2904 - val_acc: 0.3663\n",
      "Epoch 99/1000\n",
      "120/120 - 5s - loss: 2.0075 - acc: 0.3417 - val_loss: 1.2828 - val_acc: 0.3725\n",
      "Epoch 100/1000\n",
      "120/120 - 5s - loss: 1.6917 - acc: 0.3917 - val_loss: 1.2785 - val_acc: 0.3771\n",
      "Epoch 101/1000\n",
      "120/120 - 5s - loss: 1.7963 - acc: 0.3833 - val_loss: 1.2771 - val_acc: 0.3801\n",
      "Epoch 102/1000\n",
      "120/120 - 5s - loss: 1.9035 - acc: 0.3083 - val_loss: 1.2765 - val_acc: 0.3820\n",
      "Epoch 103/1000\n",
      "120/120 - 5s - loss: 1.9303 - acc: 0.3333 - val_loss: 1.2718 - val_acc: 0.3850\n",
      "Epoch 104/1000\n",
      "120/120 - 5s - loss: 1.7054 - acc: 0.3917 - val_loss: 1.2639 - val_acc: 0.3878\n",
      "Epoch 105/1000\n",
      "120/120 - 5s - loss: 1.9806 - acc: 0.3500 - val_loss: 1.2596 - val_acc: 0.3903\n",
      "Epoch 106/1000\n",
      "120/120 - 5s - loss: 1.6776 - acc: 0.3833 - val_loss: 1.2546 - val_acc: 0.3933\n",
      "Epoch 107/1000\n",
      "120/120 - 5s - loss: 1.6001 - acc: 0.4333 - val_loss: 1.2483 - val_acc: 0.3978\n",
      "Epoch 108/1000\n",
      "120/120 - 5s - loss: 1.6801 - acc: 0.4167 - val_loss: 1.2404 - val_acc: 0.4003\n",
      "Epoch 109/1000\n",
      "120/120 - 5s - loss: 2.0337 - acc: 0.3083 - val_loss: 1.2342 - val_acc: 0.4020\n",
      "Epoch 110/1000\n",
      "120/120 - 5s - loss: 1.6628 - acc: 0.4500 - val_loss: 1.2296 - val_acc: 0.4082\n",
      "Epoch 111/1000\n",
      "120/120 - 5s - loss: 1.7481 - acc: 0.3500 - val_loss: 1.2250 - val_acc: 0.4141\n",
      "Epoch 112/1000\n",
      "120/120 - 5s - loss: 1.6984 - acc: 0.3667 - val_loss: 1.2207 - val_acc: 0.4197\n",
      "Epoch 113/1000\n",
      "120/120 - 5s - loss: 1.5630 - acc: 0.4000 - val_loss: 1.2172 - val_acc: 0.4233\n",
      "Epoch 114/1000\n",
      "120/120 - 5s - loss: 1.8053 - acc: 0.3583 - val_loss: 1.2127 - val_acc: 0.4278\n",
      "Epoch 115/1000\n",
      "120/120 - 5s - loss: 1.7354 - acc: 0.3417 - val_loss: 1.2077 - val_acc: 0.4343\n",
      "Epoch 116/1000\n",
      "120/120 - 5s - loss: 1.5608 - acc: 0.4333 - val_loss: 1.2034 - val_acc: 0.4441\n",
      "Epoch 117/1000\n",
      "120/120 - 5s - loss: 1.6362 - acc: 0.3500 - val_loss: 1.1983 - val_acc: 0.4521\n",
      "Epoch 118/1000\n",
      "120/120 - 5s - loss: 1.3933 - acc: 0.4583 - val_loss: 1.1940 - val_acc: 0.4576\n",
      "Epoch 119/1000\n",
      "120/120 - 5s - loss: 1.3705 - acc: 0.4417 - val_loss: 1.1918 - val_acc: 0.4620\n",
      "Epoch 120/1000\n",
      "120/120 - 5s - loss: 1.6578 - acc: 0.3583 - val_loss: 1.1977 - val_acc: 0.4612\n",
      "Epoch 121/1000\n",
      "120/120 - 5s - loss: 1.5745 - acc: 0.4000 - val_loss: 1.1909 - val_acc: 0.4674\n",
      "Epoch 122/1000\n",
      "120/120 - 5s - loss: 1.4446 - acc: 0.4333 - val_loss: 1.1724 - val_acc: 0.4796\n",
      "Epoch 123/1000\n",
      "120/120 - 5s - loss: 1.5002 - acc: 0.4500 - val_loss: 1.1597 - val_acc: 0.4903\n",
      "Epoch 124/1000\n",
      "120/120 - 5s - loss: 1.7439 - acc: 0.3583 - val_loss: 1.1510 - val_acc: 0.4999\n",
      "Epoch 125/1000\n",
      "120/120 - 5s - loss: 1.5884 - acc: 0.4000 - val_loss: 1.1432 - val_acc: 0.5108\n",
      "Epoch 126/1000\n",
      "120/120 - 5s - loss: 1.7524 - acc: 0.3667 - val_loss: 1.1399 - val_acc: 0.5154\n",
      "Epoch 127/1000\n",
      "120/120 - 5s - loss: 1.3067 - acc: 0.5250 - val_loss: 1.1381 - val_acc: 0.5171\n",
      "Epoch 128/1000\n",
      "120/120 - 5s - loss: 1.5280 - acc: 0.3917 - val_loss: 1.1338 - val_acc: 0.5196\n",
      "Epoch 129/1000\n",
      "120/120 - 5s - loss: 1.2782 - acc: 0.4583 - val_loss: 1.1285 - val_acc: 0.5254\n",
      "Epoch 130/1000\n",
      "120/120 - 5s - loss: 1.5354 - acc: 0.4250 - val_loss: 1.1253 - val_acc: 0.5233\n",
      "Epoch 131/1000\n",
      "120/120 - 5s - loss: 1.3329 - acc: 0.4667 - val_loss: 1.1201 - val_acc: 0.5287\n",
      "Epoch 132/1000\n",
      "120/120 - 5s - loss: 1.5508 - acc: 0.3833 - val_loss: 1.1104 - val_acc: 0.5378\n",
      "Epoch 133/1000\n",
      "120/120 - 5s - loss: 1.6493 - acc: 0.3833 - val_loss: 1.1008 - val_acc: 0.5497\n",
      "Epoch 134/1000\n",
      "120/120 - 5s - loss: 1.5954 - acc: 0.4333 - val_loss: 1.0933 - val_acc: 0.5583\n",
      "Epoch 135/1000\n",
      "120/120 - 5s - loss: 1.4543 - acc: 0.3750 - val_loss: 1.0873 - val_acc: 0.5655\n",
      "Epoch 136/1000\n",
      "120/120 - 5s - loss: 1.3706 - acc: 0.5000 - val_loss: 1.0832 - val_acc: 0.5675\n",
      "Epoch 137/1000\n",
      "120/120 - 5s - loss: 1.4189 - acc: 0.5167 - val_loss: 1.0840 - val_acc: 0.5658\n",
      "Epoch 138/1000\n",
      "120/120 - 5s - loss: 1.4419 - acc: 0.4750 - val_loss: 1.0881 - val_acc: 0.5570\n",
      "Epoch 139/1000\n",
      "120/120 - 5s - loss: 1.5427 - acc: 0.3750 - val_loss: 1.0994 - val_acc: 0.5391\n",
      "Epoch 140/1000\n",
      "120/120 - 5s - loss: 1.3009 - acc: 0.4417 - val_loss: 1.0745 - val_acc: 0.5579\n",
      "Epoch 141/1000\n",
      "120/120 - 5s - loss: 1.3895 - acc: 0.4500 - val_loss: 1.0447 - val_acc: 0.5896\n",
      "Epoch 142/1000\n",
      "120/120 - 5s - loss: 1.3684 - acc: 0.4833 - val_loss: 1.0285 - val_acc: 0.6057\n",
      "Epoch 143/1000\n",
      "120/120 - 5s - loss: 1.2101 - acc: 0.5250 - val_loss: 1.0153 - val_acc: 0.6126\n",
      "Epoch 144/1000\n",
      "120/120 - 5s - loss: 1.2214 - acc: 0.5333 - val_loss: 1.0043 - val_acc: 0.6189\n",
      "Epoch 145/1000\n",
      "120/120 - 5s - loss: 1.3100 - acc: 0.5083 - val_loss: 0.9953 - val_acc: 0.6226\n",
      "Epoch 146/1000\n",
      "120/120 - 5s - loss: 1.2243 - acc: 0.5250 - val_loss: 0.9896 - val_acc: 0.6253\n",
      "Epoch 147/1000\n",
      "120/120 - 5s - loss: 1.3142 - acc: 0.5000 - val_loss: 0.9860 - val_acc: 0.6304\n",
      "Epoch 148/1000\n",
      "120/120 - 5s - loss: 1.2448 - acc: 0.5333 - val_loss: 0.9814 - val_acc: 0.6321\n",
      "Epoch 149/1000\n",
      "120/120 - 5s - loss: 1.1170 - acc: 0.5250 - val_loss: 0.9768 - val_acc: 0.6354\n",
      "Epoch 150/1000\n",
      "120/120 - 5s - loss: 1.1391 - acc: 0.5333 - val_loss: 0.9747 - val_acc: 0.6351\n",
      "Epoch 151/1000\n",
      "120/120 - 5s - loss: 1.2981 - acc: 0.4667 - val_loss: 0.9681 - val_acc: 0.6378\n",
      "Epoch 152/1000\n",
      "120/120 - 5s - loss: 1.3790 - acc: 0.4833 - val_loss: 0.9610 - val_acc: 0.6421\n",
      "Epoch 153/1000\n",
      "120/120 - 5s - loss: 1.3494 - acc: 0.4167 - val_loss: 0.9655 - val_acc: 0.6332\n",
      "Epoch 154/1000\n",
      "120/120 - 5s - loss: 1.2757 - acc: 0.4917 - val_loss: 0.9639 - val_acc: 0.6345\n",
      "Epoch 155/1000\n",
      "120/120 - 5s - loss: 1.2221 - acc: 0.5333 - val_loss: 0.9517 - val_acc: 0.6472\n",
      "Epoch 156/1000\n",
      "120/120 - 5s - loss: 1.1517 - acc: 0.5167 - val_loss: 0.9511 - val_acc: 0.6507\n",
      "Epoch 157/1000\n",
      "120/120 - 5s - loss: 1.1982 - acc: 0.6333 - val_loss: 0.9675 - val_acc: 0.6384\n",
      "Epoch 158/1000\n",
      "120/120 - 5s - loss: 1.2214 - acc: 0.5417 - val_loss: 0.9683 - val_acc: 0.6362\n",
      "Epoch 159/1000\n",
      "120/120 - 5s - loss: 1.0568 - acc: 0.5917 - val_loss: 0.9520 - val_acc: 0.6499\n",
      "Epoch 160/1000\n",
      "120/120 - 5s - loss: 1.2072 - acc: 0.5583 - val_loss: 0.9430 - val_acc: 0.6504\n",
      "Epoch 161/1000\n",
      "120/120 - 5s - loss: 1.1248 - acc: 0.5333 - val_loss: 0.9402 - val_acc: 0.6517\n",
      "Epoch 162/1000\n",
      "120/120 - 5s - loss: 1.0554 - acc: 0.5667 - val_loss: 0.9327 - val_acc: 0.6578\n",
      "Epoch 163/1000\n",
      "120/120 - 5s - loss: 1.0260 - acc: 0.5750 - val_loss: 0.9206 - val_acc: 0.6626\n",
      "Epoch 164/1000\n",
      "120/120 - 5s - loss: 1.1657 - acc: 0.5500 - val_loss: 0.9090 - val_acc: 0.6699\n",
      "Epoch 165/1000\n",
      "120/120 - 5s - loss: 1.0087 - acc: 0.6000 - val_loss: 0.8983 - val_acc: 0.6743\n",
      "Epoch 166/1000\n",
      "120/120 - 5s - loss: 1.0395 - acc: 0.5333 - val_loss: 0.8886 - val_acc: 0.6791\n",
      "Epoch 167/1000\n",
      "120/120 - 5s - loss: 1.0758 - acc: 0.5583 - val_loss: 0.8744 - val_acc: 0.6872\n",
      "Epoch 168/1000\n",
      "120/120 - 5s - loss: 1.0666 - acc: 0.6083 - val_loss: 0.8639 - val_acc: 0.6922\n",
      "Epoch 169/1000\n",
      "120/120 - 5s - loss: 1.1067 - acc: 0.5500 - val_loss: 0.8575 - val_acc: 0.6983\n",
      "Epoch 170/1000\n",
      "120/120 - 5s - loss: 1.0510 - acc: 0.5833 - val_loss: 0.8505 - val_acc: 0.7037\n",
      "Epoch 171/1000\n",
      "120/120 - 5s - loss: 1.1377 - acc: 0.5500 - val_loss: 0.8519 - val_acc: 0.7020\n",
      "Epoch 172/1000\n",
      "120/120 - 5s - loss: 0.8299 - acc: 0.7167 - val_loss: 0.8577 - val_acc: 0.6987\n",
      "Epoch 173/1000\n",
      "120/120 - 5s - loss: 1.1089 - acc: 0.5583 - val_loss: 0.8584 - val_acc: 0.6993\n",
      "Epoch 174/1000\n",
      "120/120 - 5s - loss: 1.1588 - acc: 0.5667 - val_loss: 0.8368 - val_acc: 0.7125\n",
      "Epoch 175/1000\n",
      "120/120 - 5s - loss: 1.0542 - acc: 0.6500 - val_loss: 0.8168 - val_acc: 0.7254\n",
      "Epoch 176/1000\n",
      "120/120 - 5s - loss: 1.0080 - acc: 0.5750 - val_loss: 0.8067 - val_acc: 0.7279\n",
      "Epoch 177/1000\n",
      "120/120 - 5s - loss: 1.0571 - acc: 0.6083 - val_loss: 0.7997 - val_acc: 0.7280\n",
      "Epoch 178/1000\n",
      "120/120 - 5s - loss: 0.9182 - acc: 0.6333 - val_loss: 0.7940 - val_acc: 0.7288\n",
      "Epoch 179/1000\n",
      "120/120 - 5s - loss: 0.8771 - acc: 0.6667 - val_loss: 0.7897 - val_acc: 0.7289\n",
      "Epoch 180/1000\n",
      "120/120 - 5s - loss: 0.9341 - acc: 0.5583 - val_loss: 0.7909 - val_acc: 0.7316\n",
      "Epoch 181/1000\n",
      "120/120 - 5s - loss: 0.7387 - acc: 0.7083 - val_loss: 0.7892 - val_acc: 0.7321\n",
      "Epoch 182/1000\n",
      "120/120 - 5s - loss: 0.7241 - acc: 0.6667 - val_loss: 0.7818 - val_acc: 0.7353\n",
      "Epoch 183/1000\n",
      "120/120 - 5s - loss: 0.9310 - acc: 0.6333 - val_loss: 0.7716 - val_acc: 0.7392\n",
      "Epoch 184/1000\n",
      "120/120 - 5s - loss: 0.8134 - acc: 0.6583 - val_loss: 0.7557 - val_acc: 0.7441\n",
      "Epoch 185/1000\n",
      "120/120 - 5s - loss: 0.9496 - acc: 0.6250 - val_loss: 0.7435 - val_acc: 0.7470\n",
      "Epoch 186/1000\n",
      "120/120 - 5s - loss: 0.9138 - acc: 0.6917 - val_loss: 0.7416 - val_acc: 0.7538\n",
      "Epoch 187/1000\n",
      "120/120 - 5s - loss: 0.7047 - acc: 0.6833 - val_loss: 0.7425 - val_acc: 0.7542\n",
      "Epoch 188/1000\n",
      "120/120 - 5s - loss: 0.8648 - acc: 0.7000 - val_loss: 0.7285 - val_acc: 0.7617\n",
      "Epoch 189/1000\n",
      "120/120 - 5s - loss: 0.7554 - acc: 0.6583 - val_loss: 0.7119 - val_acc: 0.7630\n",
      "Epoch 190/1000\n",
      "120/120 - 5s - loss: 0.9207 - acc: 0.6667 - val_loss: 0.7021 - val_acc: 0.7642\n",
      "Epoch 191/1000\n",
      "120/120 - 5s - loss: 0.6495 - acc: 0.7333 - val_loss: 0.6970 - val_acc: 0.7658\n",
      "Epoch 192/1000\n",
      "120/120 - 5s - loss: 0.7631 - acc: 0.6917 - val_loss: 0.6905 - val_acc: 0.7691\n",
      "Epoch 193/1000\n",
      "120/120 - 5s - loss: 0.6594 - acc: 0.7083 - val_loss: 0.6822 - val_acc: 0.7711\n",
      "Epoch 194/1000\n",
      "120/120 - 5s - loss: 0.7895 - acc: 0.6667 - val_loss: 0.6735 - val_acc: 0.7720\n",
      "Epoch 195/1000\n",
      "120/120 - 5s - loss: 0.6971 - acc: 0.7083 - val_loss: 0.6686 - val_acc: 0.7758\n",
      "Epoch 196/1000\n",
      "120/120 - 5s - loss: 0.8096 - acc: 0.7333 - val_loss: 0.6609 - val_acc: 0.7799\n",
      "Epoch 197/1000\n",
      "120/120 - 5s - loss: 0.6190 - acc: 0.7667 - val_loss: 0.6486 - val_acc: 0.7811\n",
      "Epoch 198/1000\n",
      "120/120 - 5s - loss: 0.7075 - acc: 0.7167 - val_loss: 0.6409 - val_acc: 0.7799\n",
      "Epoch 199/1000\n",
      "120/120 - 5s - loss: 0.6038 - acc: 0.8000 - val_loss: 0.6349 - val_acc: 0.7836\n",
      "Epoch 200/1000\n",
      "120/120 - 5s - loss: 0.7605 - acc: 0.7083 - val_loss: 0.6356 - val_acc: 0.7914\n",
      "Epoch 201/1000\n",
      "120/120 - 5s - loss: 0.5643 - acc: 0.8000 - val_loss: 0.6368 - val_acc: 0.7921\n",
      "Epoch 202/1000\n",
      "120/120 - 5s - loss: 0.6810 - acc: 0.7000 - val_loss: 0.6307 - val_acc: 0.7938\n",
      "Epoch 203/1000\n",
      "120/120 - 5s - loss: 0.6385 - acc: 0.7500 - val_loss: 0.6229 - val_acc: 0.7979\n",
      "Epoch 204/1000\n",
      "120/120 - 5s - loss: 0.5643 - acc: 0.7250 - val_loss: 0.6138 - val_acc: 0.7987\n",
      "Epoch 205/1000\n",
      "120/120 - 5s - loss: 0.6632 - acc: 0.7750 - val_loss: 0.6115 - val_acc: 0.7987\n",
      "Epoch 206/1000\n",
      "120/120 - 5s - loss: 0.6440 - acc: 0.7250 - val_loss: 0.6039 - val_acc: 0.8011\n",
      "Epoch 207/1000\n",
      "120/120 - 5s - loss: 0.5971 - acc: 0.7667 - val_loss: 0.5935 - val_acc: 0.8024\n",
      "Epoch 208/1000\n",
      "120/120 - 5s - loss: 0.4771 - acc: 0.8250 - val_loss: 0.5858 - val_acc: 0.8026\n",
      "Epoch 209/1000\n",
      "120/120 - 5s - loss: 0.5502 - acc: 0.7750 - val_loss: 0.5786 - val_acc: 0.8042\n",
      "Epoch 210/1000\n",
      "120/120 - 5s - loss: 0.5622 - acc: 0.7917 - val_loss: 0.5747 - val_acc: 0.8096\n",
      "Epoch 211/1000\n",
      "120/120 - 5s - loss: 0.5021 - acc: 0.7833 - val_loss: 0.5685 - val_acc: 0.8112\n",
      "Epoch 212/1000\n",
      "120/120 - 5s - loss: 0.4142 - acc: 0.8750 - val_loss: 0.5605 - val_acc: 0.8114\n",
      "Epoch 213/1000\n",
      "120/120 - 5s - loss: 0.5190 - acc: 0.8083 - val_loss: 0.5531 - val_acc: 0.8072\n",
      "Epoch 214/1000\n",
      "120/120 - 5s - loss: 0.5420 - acc: 0.8333 - val_loss: 0.5473 - val_acc: 0.8083\n",
      "Epoch 215/1000\n",
      "120/120 - 5s - loss: 0.4163 - acc: 0.8667 - val_loss: 0.5404 - val_acc: 0.8128\n",
      "Epoch 216/1000\n",
      "120/120 - 5s - loss: 0.4373 - acc: 0.8167 - val_loss: 0.5360 - val_acc: 0.8141\n",
      "Epoch 217/1000\n",
      "120/120 - 5s - loss: 0.4187 - acc: 0.8917 - val_loss: 0.5317 - val_acc: 0.8155\n",
      "Epoch 218/1000\n",
      "120/120 - 5s - loss: 0.4477 - acc: 0.8667 - val_loss: 0.5269 - val_acc: 0.8155\n",
      "Epoch 219/1000\n",
      "120/120 - 5s - loss: 0.3932 - acc: 0.8583 - val_loss: 0.5212 - val_acc: 0.8187\n",
      "Epoch 220/1000\n",
      "120/120 - 5s - loss: 0.3040 - acc: 0.9000 - val_loss: 0.5203 - val_acc: 0.8272\n",
      "Epoch 221/1000\n",
      "120/120 - 5s - loss: 0.5480 - acc: 0.8250 - val_loss: 0.5168 - val_acc: 0.8274\n",
      "Epoch 222/1000\n",
      "120/120 - 5s - loss: 0.3715 - acc: 0.8833 - val_loss: 0.5082 - val_acc: 0.8278\n",
      "Epoch 223/1000\n",
      "120/120 - 5s - loss: 0.3041 - acc: 0.8917 - val_loss: 0.5022 - val_acc: 0.8289\n",
      "Epoch 224/1000\n",
      "120/120 - 5s - loss: 0.4251 - acc: 0.8500 - val_loss: 0.4985 - val_acc: 0.8304\n",
      "Epoch 225/1000\n",
      "120/120 - 5s - loss: 0.4063 - acc: 0.8583 - val_loss: 0.4973 - val_acc: 0.8304\n",
      "Epoch 226/1000\n",
      "120/120 - 5s - loss: 0.4355 - acc: 0.8750 - val_loss: 0.4967 - val_acc: 0.8320\n",
      "Epoch 227/1000\n",
      "120/120 - 5s - loss: 0.3476 - acc: 0.8917 - val_loss: 0.4939 - val_acc: 0.8339\n",
      "Epoch 228/1000\n",
      "120/120 - 5s - loss: 0.3441 - acc: 0.8417 - val_loss: 0.4859 - val_acc: 0.8341\n",
      "Epoch 229/1000\n",
      "120/120 - 5s - loss: 0.3273 - acc: 0.8917 - val_loss: 0.4798 - val_acc: 0.8311\n",
      "Epoch 230/1000\n",
      "120/120 - 5s - loss: 0.3796 - acc: 0.8500 - val_loss: 0.4764 - val_acc: 0.8305\n",
      "Epoch 231/1000\n",
      "120/120 - 5s - loss: 0.3253 - acc: 0.8833 - val_loss: 0.4710 - val_acc: 0.8336\n",
      "Epoch 232/1000\n",
      "120/120 - 5s - loss: 0.2012 - acc: 0.9250 - val_loss: 0.4666 - val_acc: 0.8399\n",
      "Epoch 233/1000\n",
      "120/120 - 5s - loss: 0.2615 - acc: 0.9000 - val_loss: 0.4641 - val_acc: 0.8432\n",
      "Epoch 234/1000\n",
      "120/120 - 5s - loss: 0.3141 - acc: 0.8750 - val_loss: 0.4591 - val_acc: 0.8428\n",
      "Epoch 235/1000\n",
      "120/120 - 5s - loss: 0.2433 - acc: 0.8917 - val_loss: 0.4571 - val_acc: 0.8391\n",
      "Epoch 236/1000\n",
      "120/120 - 5s - loss: 0.2060 - acc: 0.9500 - val_loss: 0.4580 - val_acc: 0.8372\n",
      "Epoch 237/1000\n",
      "120/120 - 5s - loss: 0.2325 - acc: 0.9500 - val_loss: 0.4603 - val_acc: 0.8342\n",
      "Epoch 238/1000\n",
      "120/120 - 5s - loss: 0.1806 - acc: 0.9500 - val_loss: 0.4590 - val_acc: 0.8338\n",
      "Epoch 239/1000\n",
      "120/120 - 5s - loss: 0.2340 - acc: 0.9333 - val_loss: 0.4516 - val_acc: 0.8386\n",
      "Epoch 240/1000\n",
      "120/120 - 5s - loss: 0.2445 - acc: 0.9000 - val_loss: 0.4468 - val_acc: 0.8442\n",
      "Epoch 241/1000\n",
      "120/120 - 5s - loss: 0.1741 - acc: 0.9583 - val_loss: 0.4456 - val_acc: 0.8480\n",
      "Epoch 242/1000\n",
      "120/120 - 5s - loss: 0.2273 - acc: 0.9250 - val_loss: 0.4457 - val_acc: 0.8483\n",
      "Epoch 243/1000\n",
      "120/120 - 5s - loss: 0.2154 - acc: 0.9167 - val_loss: 0.4440 - val_acc: 0.8493\n",
      "Epoch 244/1000\n",
      "120/120 - 5s - loss: 0.2294 - acc: 0.9250 - val_loss: 0.4407 - val_acc: 0.8492\n",
      "Epoch 245/1000\n",
      "120/120 - 5s - loss: 0.1864 - acc: 0.9417 - val_loss: 0.4379 - val_acc: 0.8489\n",
      "Epoch 246/1000\n",
      "120/120 - 5s - loss: 0.2397 - acc: 0.9167 - val_loss: 0.4361 - val_acc: 0.8489\n",
      "Epoch 247/1000\n",
      "120/120 - 5s - loss: 0.1706 - acc: 0.9500 - val_loss: 0.4340 - val_acc: 0.8486\n",
      "Epoch 248/1000\n",
      "120/120 - 5s - loss: 0.1981 - acc: 0.9250 - val_loss: 0.4310 - val_acc: 0.8505\n",
      "Epoch 249/1000\n",
      "120/120 - 5s - loss: 0.1777 - acc: 0.9667 - val_loss: 0.4286 - val_acc: 0.8512\n",
      "Epoch 250/1000\n",
      "120/120 - 5s - loss: 0.1527 - acc: 0.9750 - val_loss: 0.4263 - val_acc: 0.8524\n",
      "Epoch 251/1000\n",
      "120/120 - 5s - loss: 0.1546 - acc: 0.9583 - val_loss: 0.4247 - val_acc: 0.8547\n",
      "Epoch 252/1000\n",
      "120/120 - 5s - loss: 0.1489 - acc: 0.9417 - val_loss: 0.4228 - val_acc: 0.8568\n",
      "Epoch 253/1000\n",
      "120/120 - 5s - loss: 0.1502 - acc: 0.9667 - val_loss: 0.4212 - val_acc: 0.8571\n",
      "Epoch 254/1000\n",
      "120/120 - 5s - loss: 0.1677 - acc: 0.9417 - val_loss: 0.4205 - val_acc: 0.8564\n",
      "Epoch 255/1000\n",
      "120/120 - 5s - loss: 0.1114 - acc: 0.9750 - val_loss: 0.4201 - val_acc: 0.8551\n",
      "Epoch 256/1000\n",
      "120/120 - 5s - loss: 0.1452 - acc: 0.9750 - val_loss: 0.4197 - val_acc: 0.8559\n",
      "Epoch 257/1000\n",
      "120/120 - 5s - loss: 0.1317 - acc: 0.9667 - val_loss: 0.4194 - val_acc: 0.8561\n",
      "Epoch 258/1000\n",
      "120/120 - 5s - loss: 0.1190 - acc: 0.9750 - val_loss: 0.4192 - val_acc: 0.8558\n",
      "Epoch 259/1000\n",
      "120/120 - 5s - loss: 0.1015 - acc: 0.9750 - val_loss: 0.4188 - val_acc: 0.8558\n",
      "Epoch 260/1000\n",
      "120/120 - 5s - loss: 0.0949 - acc: 0.9667 - val_loss: 0.4184 - val_acc: 0.8561\n",
      "Epoch 261/1000\n",
      "120/120 - 5s - loss: 0.0815 - acc: 0.9833 - val_loss: 0.4188 - val_acc: 0.8564\n",
      "Epoch 262/1000\n",
      "120/120 - 5s - loss: 0.0728 - acc: 0.9917 - val_loss: 0.4190 - val_acc: 0.8568\n",
      "Epoch 263/1000\n",
      "120/120 - 5s - loss: 0.0668 - acc: 0.9917 - val_loss: 0.4192 - val_acc: 0.8563\n",
      "Epoch 264/1000\n",
      "120/120 - 5s - loss: 0.0970 - acc: 0.9833 - val_loss: 0.4186 - val_acc: 0.8562\n",
      "Epoch 265/1000\n",
      "120/120 - 5s - loss: 0.0574 - acc: 0.9917 - val_loss: 0.4177 - val_acc: 0.8578\n",
      "Epoch 266/1000\n",
      "120/120 - 5s - loss: 0.1107 - acc: 0.9750 - val_loss: 0.4168 - val_acc: 0.8582\n",
      "Epoch 267/1000\n",
      "120/120 - 5s - loss: 0.1052 - acc: 0.9750 - val_loss: 0.4162 - val_acc: 0.8583\n",
      "Epoch 268/1000\n",
      "120/120 - 5s - loss: 0.0861 - acc: 0.9750 - val_loss: 0.4160 - val_acc: 0.8588\n",
      "Epoch 269/1000\n",
      "120/120 - 5s - loss: 0.1018 - acc: 0.9833 - val_loss: 0.4157 - val_acc: 0.8599\n",
      "Epoch 270/1000\n",
      "120/120 - 5s - loss: 0.0922 - acc: 0.9750 - val_loss: 0.4150 - val_acc: 0.8596\n",
      "Epoch 271/1000\n",
      "120/120 - 5s - loss: 0.0659 - acc: 1.0000 - val_loss: 0.4144 - val_acc: 0.8603\n",
      "Epoch 272/1000\n",
      "120/120 - 5s - loss: 0.1066 - acc: 0.9667 - val_loss: 0.4144 - val_acc: 0.8603\n",
      "Epoch 273/1000\n",
      "120/120 - 5s - loss: 0.0701 - acc: 0.9917 - val_loss: 0.4150 - val_acc: 0.8600\n",
      "Epoch 274/1000\n",
      "120/120 - 5s - loss: 0.0698 - acc: 1.0000 - val_loss: 0.4154 - val_acc: 0.8599\n",
      "Epoch 275/1000\n",
      "120/120 - 5s - loss: 0.0588 - acc: 1.0000 - val_loss: 0.4155 - val_acc: 0.8601\n",
      "Epoch 276/1000\n",
      "120/120 - 7s - loss: 0.0989 - acc: 0.9583 - val_loss: 0.4157 - val_acc: 0.8604\n"
     ]
    }
   ],
   "source": [
    "log = model.fit([train_input_ids, train_input_masks, train_segment_ids],\n",
    "                train_labels, validation_data=test_set,\n",
    "                verbose=2, callbacks=callbacks_list,\n",
    "                epochs=1000, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600/7600 - 10s - loss: 0.4143 - acc: 0.8603\n",
      "Loss: 0.4143489755454816 Acc: 0.86026317\n"
     ]
    }
   ],
   "source": [
    "[eval_loss, eval_acc] = model.evaluate([test_input_ids, test_input_masks, test_segment_ids], test_labels, verbose=2, batch_size=256)\n",
    "print(\"Loss:\", eval_loss, \"Acc:\", eval_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training set from: /home/jovyan/.keras/datasets/ag_news\n",
      "Examples: 120000 Classes: 4\n"
     ]
    }
   ],
   "source": [
    "train_text, train_label, num_classes = bert.utils.load_ag_news_dataset(max_seq_len=MAX_SEQ_LEN,\n",
    "                                                                       test=False)\n",
    "\n",
    "train_label = np.asarray(train_label)\n",
    "feat = bert.model.convert_text_to_features(tokenizer, train_text, train_label, max_seq_length=MAX_SEQ_LEN, verbose=False)\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = feat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlen_list = []\\nfor example_mask in train_input_masks.tolist():\\n    len_list.append(np.sum(example_mask))\\nprint(stats.describe(np.asarray(len_list)))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "len_list = []\n",
    "for example_mask in train_input_masks.tolist():\n",
    "    len_list.append(np.sum(example_mask))\n",
    "print(stats.describe(np.asarray(len_list)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000/120000 - 71s\n",
      "120000/120000 - 69s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n"
     ]
    }
   ],
   "source": [
    "y_softmax_list = []\n",
    "\n",
    "num_iterations = 30\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    with tf.keras.backend.learning_phase_scope(1):\n",
    "        y_pred = model.predict([train_input_ids, train_input_masks, train_segment_ids], verbose=2, batch_size=256)\n",
    "    y_softmax_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold < 0.6\n",
      "Correct wrong: 7128\n",
      "Incorrect wrong: 7744\n",
      "Useful effort: 0.47928994082840237\n",
      "Overall accuracy: 0.8627666666666667\n"
     ]
    }
   ],
   "source": [
    "agg_y_softmax = np.stack(y_softmax_list, axis=-2)\n",
    "\n",
    "probs = np.sum(agg_y_softmax, axis=1)/num_iterations\n",
    "probs = probs.tolist()\n",
    "\n",
    "margin_list = []\n",
    "\n",
    "threshold = 0.6\n",
    "\n",
    "false_positive = 0\n",
    "positive = 0\n",
    "\n",
    "for i, prob in enumerate(probs):\n",
    "    margin = max(prob) - min(prob)\n",
    "    if margin < threshold:\n",
    "        margin_list.append(margin)\n",
    "        pred = np.argmax(prob)\n",
    "        if pred == train_label[i]:\n",
    "            false_positive += 1\n",
    "        else:\n",
    "            positive += 1\n",
    "            \n",
    "print(\"Threshold <\", threshold)\n",
    "print(\"Correct wrong:\", positive)\n",
    "print(\"Incorrect wrong:\", false_positive)\n",
    "print(\"Useful effort:\", positive/(positive+false_positive))\n",
    "    \n",
    "acc = []\n",
    "\n",
    "for i, prob in enumerate(probs):\n",
    "    pred = np.argmax(prob)\n",
    "    if pred == train_label[i]:\n",
    "        acc.append(1)\n",
    "    else:\n",
    "        acc.append(0)\n",
    "        \n",
    "acc = sum(acc)/len(probs)\n",
    "print(\"Overall accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
