{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Training\n",
    "\n",
    "Single GPU BERT Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU selection\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.compat.v1.keras import layers\n",
    "\n",
    "import utils\n",
    "import bert_utils\n",
    "import bert_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTLARGE       = False\n",
    "USE_AMP         = True\n",
    "USE_XLA         = True\n",
    "MAX_SEQ_LEN     = 128\n",
    "LEARNING_RATE   = 2e-5\n",
    "TUNE_LAYERS     = -1\n",
    "DATASET_PORTION = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BERTLARGE:\n",
    "    BERT_PATH = \"https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1\"\n",
    "    H_SIZE = 1024\n",
    "else:\n",
    "    BERT_PATH = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "    H_SIZE = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\"\n",
    "os.environ[\"TF_GPU_THREAD_COUNT\"] = \"2\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "if USE_XLA:\n",
    "    opt_level = tf.OptimizerOptions.ON_1\n",
    "    tf.enable_resource_variables()\n",
    "else:\n",
    "    opt_level = tf.OptimizerOptions.OFF\n",
    "config.graph_options.optimizer_options.global_jit_level = opt_level\n",
    "config.graph_options.rewrite_options.auto_mixed_precision = USE_AMP\n",
    "sess = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = bert_utils.create_tokenizer_from_hub_module(BERT_PATH, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features:  10%|█         | 122/1200 [00:00<00:00, 1214.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training set from: /home/jovyan/.keras/datasets/ag_news\n",
      "Examples: 120000 Classes: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 100%|██████████| 1200/1200 [00:00<00:00, 1227.60it/s]\n"
     ]
    }
   ],
   "source": [
    "train_text, train_label, num_classes = utils.load_ag_news_dataset(max_seq_len=MAX_SEQ_LEN,\n",
    "                                                                  test=False)\n",
    "\n",
    "if DATASET_PORTION < 1:\n",
    "    num_examples = int(len(train_label) * DATASET_PORTION)\n",
    "    _, train_text, _, train_label= train_test_split(train_text, train_label, test_size=DATASET_PORTION, stratify=train_label)\n",
    "else:\n",
    "    num_examples = len(train_label)\n",
    "\n",
    "train_label = np.asarray(train_label)\n",
    "num_examples = len(train_label)\n",
    "train_examples = bert_utils.convert_text_to_examples(train_text, train_label)\n",
    "feat = bert_utils.convert_examples_to_features(tokenizer,\n",
    "                                               train_examples,\n",
    "                                               max_seq_length=MAX_SEQ_LEN,\n",
    "                                               verbose=True)\n",
    "\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = feat\n",
    "\n",
    "train_input_ids, train_input_masks, train_segment_ids, train_labels = shuffle(train_input_ids,\n",
    "                                                                              train_input_masks,\n",
    "                                                                              train_segment_ids,\n",
    "                                                                              train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features:   2%|▏         | 126/7600 [00:00<00:05, 1250.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test set from: /home/jovyan/.keras/datasets/ag_news\n",
      "Examples: 7600 Classes: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 100%|██████████| 7600/7600 [00:06<00:00, 1228.91it/s]\n"
     ]
    }
   ],
   "source": [
    "examples, labels, num_classes = utils.load_ag_news_dataset(max_seq_len=MAX_SEQ_LEN,\n",
    "                                                           test=True)\n",
    "labels = np.asarray(labels)\n",
    "test_examples = bert_utils.convert_text_to_examples(examples, labels)\n",
    "feat = bert_utils.convert_examples_to_features(tokenizer,\n",
    "                                               test_examples,\n",
    "                                               max_seq_length=MAX_SEQ_LEN,\n",
    "                                               verbose=True)\n",
    "\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels) = feat\n",
    "\n",
    "test_input_ids, test_input_masks, test_segment_ids, test_labels = shuffle(test_input_ids,\n",
    "                                                                          test_input_masks,\n",
    "                                                                          test_segment_ids,\n",
    "                                                                          test_labels)\n",
    "\n",
    "test_set = ([test_input_ids, test_input_masks, test_segment_ids], test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(tf.keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_AMP:\n",
    "    tf.keras.mixed_precision.experimental.set_policy('infer_float32_vars')\n",
    "\n",
    "in_id = layers.Input(shape=(MAX_SEQ_LEN,), name=\"input_ids\")\n",
    "in_mask = layers.Input(shape=(MAX_SEQ_LEN,), name=\"input_masks\")\n",
    "in_segment = layers.Input(shape=(MAX_SEQ_LEN,), name=\"segment_ids\")\n",
    "\n",
    "in_bert = [in_id, in_mask, in_segment]\n",
    "l_bert = bert_utils.BERT(fine_tune_layers=TUNE_LAYERS,\n",
    "                         bert_path=BERT_PATH,\n",
    "                         return_sequence=False,\n",
    "                         output_size=H_SIZE,\n",
    "                         debug=False)(in_bert)\n",
    "l_drop = MCDropout(rate=0.5)(l_bert)\n",
    "out_pred = layers.Dense(num_classes, activation=\"softmax\")(l_drop)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=in_bert, outputs=out_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "opt = bert_optimizer.RAdam(lr=LEARNING_RATE)\n",
    "\n",
    "if USE_AMP:\n",
    "    opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, \"dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "mc_dropout (MCDropout)          (None, 768)          0           bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 4)            3076        mc_dropout[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 110,107,966\n",
      "Trainable params: 109,485,316\n",
      "Non-trainable params: 622,650\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    warmup_steps = 26000\n",
    "    warmup_epochs = warmup_steps//num_examples\n",
    "    if epoch < warmup_epochs:\n",
    "        return LEARNING_RATE*(epoch/warmup_epochs)\n",
    "    else:\n",
    "        return LEARNING_RATE\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks_list = [lr_schedule, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200 samples, validate on 7600 samples\n",
      "Epoch 1/1000\n",
      "1200/1200 - 70s - loss: 1.8929 - acc: 0.2608 - val_loss: 1.9099 - val_acc: 0.2563\n",
      "Epoch 2/1000\n",
      "1200/1200 - 41s - loss: 1.9078 - acc: 0.2575 - val_loss: 1.8690 - val_acc: 0.2617\n",
      "Epoch 3/1000\n",
      "1200/1200 - 8s - loss: 1.8701 - acc: 0.2400 - val_loss: 1.7860 - val_acc: 0.2654\n",
      "Epoch 4/1000\n",
      "1200/1200 - 8s - loss: 1.7478 - acc: 0.2675 - val_loss: 1.6429 - val_acc: 0.2947\n",
      "Epoch 5/1000\n",
      "1200/1200 - 8s - loss: 1.5300 - acc: 0.3258 - val_loss: 1.5222 - val_acc: 0.3259\n",
      "Epoch 6/1000\n",
      "1200/1200 - 8s - loss: 1.4732 - acc: 0.3450 - val_loss: 1.4603 - val_acc: 0.3475\n",
      "Epoch 7/1000\n",
      "1200/1200 - 8s - loss: 1.4466 - acc: 0.3692 - val_loss: 1.3668 - val_acc: 0.3957\n",
      "Epoch 8/1000\n",
      "1200/1200 - 8s - loss: 1.3124 - acc: 0.4383 - val_loss: 1.2634 - val_acc: 0.4471\n",
      "Epoch 9/1000\n",
      "1200/1200 - 8s - loss: 1.2205 - acc: 0.4717 - val_loss: 1.1633 - val_acc: 0.5014\n",
      "Epoch 10/1000\n",
      "1200/1200 - 8s - loss: 1.0657 - acc: 0.5625 - val_loss: 1.0229 - val_acc: 0.5763\n",
      "Epoch 11/1000\n",
      "1200/1200 - 8s - loss: 0.9164 - acc: 0.6342 - val_loss: 0.8807 - val_acc: 0.6488\n",
      "Epoch 12/1000\n",
      "1200/1200 - 8s - loss: 0.7270 - acc: 0.7408 - val_loss: 0.7071 - val_acc: 0.7513\n",
      "Epoch 13/1000\n",
      "1200/1200 - 8s - loss: 0.5674 - acc: 0.8133 - val_loss: 0.5791 - val_acc: 0.8055\n",
      "Epoch 14/1000\n",
      "1200/1200 - 8s - loss: 0.4517 - acc: 0.8600 - val_loss: 0.4912 - val_acc: 0.8397\n",
      "Epoch 15/1000\n",
      "1200/1200 - 8s - loss: 0.3706 - acc: 0.8942 - val_loss: 0.4368 - val_acc: 0.8546\n",
      "Epoch 16/1000\n",
      "1200/1200 - 8s - loss: 0.3080 - acc: 0.9075 - val_loss: 0.4062 - val_acc: 0.8649\n",
      "Epoch 17/1000\n",
      "1200/1200 - 8s - loss: 0.2473 - acc: 0.9367 - val_loss: 0.4003 - val_acc: 0.8636\n",
      "Epoch 18/1000\n",
      "1200/1200 - 8s - loss: 0.1985 - acc: 0.9458 - val_loss: 0.3832 - val_acc: 0.8709\n",
      "Epoch 19/1000\n",
      "1200/1200 - 8s - loss: 0.1617 - acc: 0.9592 - val_loss: 0.3757 - val_acc: 0.8759\n",
      "Epoch 20/1000\n",
      "1200/1200 - 8s - loss: 0.1301 - acc: 0.9708 - val_loss: 0.3892 - val_acc: 0.8688\n",
      "Epoch 21/1000\n",
      "1200/1200 - 8s - loss: 0.0972 - acc: 0.9808 - val_loss: 0.3699 - val_acc: 0.8778\n",
      "Epoch 22/1000\n",
      "1200/1200 - 8s - loss: 0.0671 - acc: 0.9875 - val_loss: 0.3773 - val_acc: 0.8808\n",
      "Epoch 23/1000\n",
      "1200/1200 - 8s - loss: 0.0526 - acc: 0.9925 - val_loss: 0.3982 - val_acc: 0.8779\n",
      "Epoch 24/1000\n",
      "1200/1200 - 8s - loss: 0.0350 - acc: 0.9950 - val_loss: 0.3887 - val_acc: 0.8803\n",
      "Epoch 25/1000\n",
      "1200/1200 - 8s - loss: 0.0279 - acc: 0.9975 - val_loss: 0.3985 - val_acc: 0.8803\n",
      "Epoch 26/1000\n",
      "1200/1200 - 11s - loss: 0.0238 - acc: 0.9983 - val_loss: 0.4018 - val_acc: 0.8829\n"
     ]
    }
   ],
   "source": [
    "log = model.fit([train_input_ids, train_input_masks, train_segment_ids],\n",
    "                train_labels, validation_data=test_set,\n",
    "                workers=4, use_multiprocessing=True,\n",
    "                verbose=2, callbacks=callbacks_list,\n",
    "                epochs=1000, batch_size=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7600/7600 - 10s - loss: 0.3699 - acc: 0.8786\n",
      "Loss: 0.36994663483218143 Acc: 0.8785526\n"
     ]
    }
   ],
   "source": [
    "[eval_loss, eval_acc] = model.evaluate([test_input_ids, test_input_masks, test_segment_ids], test_labels, verbose=2, batch_size=256)\n",
    "print(\"Loss:\", eval_loss, \"Acc:\", eval_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training set from: /home/jovyan/.keras/datasets/ag_news\n",
      "Examples: 120000 Classes: 4\n"
     ]
    }
   ],
   "source": [
    "train_text, train_label, num_classes = utils.load_ag_news_dataset(max_seq_len=MAX_SEQ_LEN,\n",
    "                                                                  test=False)\n",
    "\n",
    "train_label = np.asarray(train_label)\n",
    "train_examples = bert_utils.convert_text_to_examples(train_text, train_label)\n",
    "\n",
    "feat = bert_utils.convert_examples_to_features(tokenizer,\n",
    "                                               train_examples,\n",
    "                                               max_seq_length=MAX_SEQ_LEN,\n",
    "                                               verbose=False)\n",
    "\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = feat   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000/120000 - 71s\n",
      "120000/120000 - 69s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n",
      "120000/120000 - 64s\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "\n",
    "for _ in range(10):\n",
    "    y_pred = model.predict([train_input_ids, train_input_masks, train_segment_ids], verbose=2, batch_size=256)\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "    y_pred_list.append(y_pred_class)\n",
    "    \n",
    "agg_y_pred = np.stack(y_pred_list, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:22<00:00, 5280.89it/s]\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "corr_guess = 0\n",
    "wrong = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "\n",
    "for i, truth in tqdm(enumerate(train_label), total=len(train_label)):\n",
    "    pred = agg_y_pred[i, :]\n",
    "    m = stats.mode(pred).mode\n",
    "    correct = truth==m\n",
    "    \n",
    "    diff = 0\n",
    "    for item in pred:\n",
    "        if item != m:\n",
    "            diff += 1\n",
    "            \n",
    "    if diff > 5:\n",
    "        same = False\n",
    "    else:\n",
    "        same = True\n",
    "    \n",
    "    if correct and same:\n",
    "        # correct guess as correct\n",
    "        acc += 1\n",
    "    if correct and not same:\n",
    "        # wrong guess as wrong\n",
    "        false_pos += 1\n",
    "        wrong += 1\n",
    "    if not correct and same:\n",
    "        # wrong guess as correct\n",
    "        false_neg += 1\n",
    "        wrong += 1\n",
    "    if not correct and not same:\n",
    "        # correct guess as wrong\n",
    "        corr_guess += 1\n",
    "        acc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct guesses: 106786\n",
      "Wrong guesses: 13214 ; 11.0 %\n",
      "Correct guess as wrong: 101\n",
      "Wrong guess as wrong: 55\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct guesses:\", acc)\n",
    "print(\"Wrong guesses:\", wrong, \";\", round(wrong/120000*100,1), \"%\")\n",
    "print(\"Correct guess as wrong:\", corr_guess)\n",
    "print(\"Wrong guess as wrong:\", false_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
