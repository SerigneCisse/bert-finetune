{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU selection\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "from tensorflow.compat.v1.keras import layers\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import bert.model\n",
    "import bert.utils\n",
    "import bert.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTLARGE     = False\n",
    "USE_AMP       = True\n",
    "USE_XLA       = True\n",
    "MAX_SEQ_LEN   = 128\n",
    "LEARNING_RATE = 1e-5\n",
    "TUNE_LAYERS   = -1\n",
    "DROPOUT_RATE  = 0.5\n",
    "BATCH_SIZE    = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PORTION = 0.005\n",
    "\n",
    "if BERTLARGE:\n",
    "    BERT_PATH = \"https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1\"\n",
    "    H_SIZE = 1024\n",
    "else:\n",
    "    BERT_PATH = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "    H_SIZE = 768\n",
    "\n",
    "sess = tf.Session()\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = bert.model.create_tokenizer_from_hub_module(BERT_PATH, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, train_label, num_classes = bert.utils.load_ag_news_dataset(max_seq_len=MAX_SEQ_LEN,\n",
    "                                                                       test=False)\n",
    "\n",
    "if DATASET_PORTION < 1:\n",
    "    num_examples = int(len(train_label) * DATASET_PORTION)\n",
    "    _, train_text, _, train_label= train_test_split(train_text, train_label, test_size=DATASET_PORTION, stratify=train_label)\n",
    "else:\n",
    "    num_examples = len(train_label)\n",
    "\n",
    "train_label = np.asarray(train_label)\n",
    "feat = bert.model.convert_text_to_features(tokenizer, train_text, train_label, max_seq_length=MAX_SEQ_LEN, verbose=False)\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = feat\n",
    "\n",
    "print(\"Number of training examples:\", len(train_labels))\n",
    "\n",
    "examples, labels, num_classes = bert.utils.load_ag_news_dataset(max_seq_len=MAX_SEQ_LEN,\n",
    "                                                                test=True)\n",
    "labels = np.asarray(labels)\n",
    "feat = bert.model.convert_text_to_features(tokenizer, examples, labels, max_seq_length=MAX_SEQ_LEN, verbose=False)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels) = feat\n",
    "\n",
    "test_input_ids, test_input_masks, test_segment_ids, test_labels = shuffle(test_input_ids,\n",
    "                                                                          test_input_masks,\n",
    "                                                                          test_segment_ids,\n",
    "                                                                          test_labels)\n",
    "\n",
    "test_set = ([test_input_ids, test_input_masks, test_segment_ids], test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    tf.keras.backend.clear_session()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    if USE_XLA:\n",
    "        opt_level = tf.OptimizerOptions.ON_1\n",
    "        tf.enable_resource_variables()\n",
    "    else:\n",
    "        opt_level = tf.OptimizerOptions.OFF\n",
    "    config.graph_options.optimizer_options.global_jit_level = opt_level\n",
    "    config.graph_options.rewrite_options.auto_mixed_precision = USE_AMP\n",
    "    sess = tf.Session(config=config)\n",
    "    tf.keras.backend.set_session(sess)\n",
    "    \n",
    "    if USE_AMP:\n",
    "        tf.keras.mixed_precision.experimental.set_policy('infer_float32_vars')\n",
    "\n",
    "    in_id = layers.Input(shape=(MAX_SEQ_LEN,), name=\"input_ids\")\n",
    "    in_mask = layers.Input(shape=(MAX_SEQ_LEN,), name=\"input_masks\")\n",
    "    in_segment = layers.Input(shape=(MAX_SEQ_LEN,), name=\"segment_ids\")\n",
    "    in_bert = [in_id, in_mask, in_segment]\n",
    "    l_bert = bert.model.BERT(fine_tune_layers=TUNE_LAYERS,\n",
    "                             bert_path=BERT_PATH,\n",
    "                             return_sequence=True,\n",
    "                             output_size=H_SIZE,\n",
    "                             debug=False)(in_bert)\n",
    "    l_bert = layers.Reshape((MAX_SEQ_LEN, H_SIZE))(l_bert)\n",
    "    l_drop_1 = layers.SpatialDropout1D(rate=DROPOUT_RATE)(l_bert)\n",
    "    l_conv = layers.Conv1D(H_SIZE//2, 1)(l_drop_1)\n",
    "    l_flat = layers.Flatten()(l_conv)\n",
    "    l_drop_2 = layers.Dropout(rate=DROPOUT_RATE)(l_flat)\n",
    "    out_pred = layers.Dense(num_classes, activation=\"softmax\")(l_drop_2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=in_bert, outputs=out_pred)\n",
    "\n",
    "    opt = bert.optimizer.RAdam(lr=LEARNING_RATE)\n",
    "\n",
    "    if USE_AMP:\n",
    "        opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, \"dynamic\")\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=opt,\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    warmup_steps = 26000\n",
    "    warmup_epochs = warmup_steps//num_examples\n",
    "    if epoch < warmup_epochs:\n",
    "        return LEARNING_RATE*(epoch/warmup_epochs)\n",
    "    else:\n",
    "        return LEARNING_RATE\n",
    "    \n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks_list = [lr_schedule, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = model.fit([train_input_ids, train_input_masks, train_segment_ids],\n",
    "                train_labels, validation_data=test_set,\n",
    "                verbose=2, callbacks=callbacks_list,\n",
    "                epochs=1000, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[eval_loss, eval_acc] = model.evaluate([test_input_ids, test_input_masks, test_segment_ids], test_labels, verbose=2, batch_size=256)\n",
    "print(\"Loss:\", eval_loss, \"Acc:\", eval_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, train_label, num_classes = bert.utils.load_ag_news_dataset(max_seq_len=MAX_SEQ_LEN,\n",
    "                                                                       test=False)\n",
    "\n",
    "train_label = np.asarray(train_label)\n",
    "feat = bert.model.convert_text_to_features(tokenizer, train_text, train_label, max_seq_length=MAX_SEQ_LEN, verbose=False)\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = feat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_softmax_list = []\n",
    "\n",
    "num_iterations = 10\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    with tf.keras.backend.learning_phase_scope(1):\n",
    "        y_pred = model.predict([train_input_ids, train_input_masks, train_segment_ids], verbose=2, batch_size=256)\n",
    "    y_softmax_list.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_y_softmax = np.stack(y_softmax_list, axis=-2)\n",
    "\n",
    "probs = np.sum(agg_y_softmax, axis=1)/num_iterations\n",
    "probs = probs.tolist()\n",
    "\n",
    "margin_list = []\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "false_positive = 0\n",
    "positive = 0\n",
    "\n",
    "for i, prob in enumerate(probs):\n",
    "    # calculate a margin:\n",
    "    # top_pred - (sum of other preds)\n",
    "    copy_prob = copy.deepcopy(prob)\n",
    "    copy_prob.sort()\n",
    "    margin = copy_prob[-1] - copy_prob[0]\n",
    "    if margin < threshold:\n",
    "        margin_list.append(margin)\n",
    "        pred = np.argmax(prob)\n",
    "        if pred == train_label[i]:\n",
    "            false_positive += 1\n",
    "        else:\n",
    "            positive += 1\n",
    "            \n",
    "print(\"Threshold <\", threshold)\n",
    "print(\"Correct wrong:\", positive)\n",
    "print(\"Incorrect wrong:\", false_positive)\n",
    "print(\"Useful effort:\", positive/(positive+false_positive))\n",
    "    \n",
    "acc = []\n",
    "\n",
    "for i, prob in enumerate(probs):\n",
    "    pred = np.argmax(prob)\n",
    "    if pred == train_label[i]:\n",
    "        acc.append(1)\n",
    "    else:\n",
    "        acc.append(0)\n",
    "        \n",
    "acc = sum(acc)/len(probs)\n",
    "print(\"Overall accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
